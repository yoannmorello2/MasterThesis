{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49348f71",
   "metadata": {},
   "source": [
    "# Arithmetic Trees Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c2c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''arithmetic trees generators\n",
    "can output  nltk trees or array  representation'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from math import sqrt\n",
    "from numpy import argmax\n",
    "from math import floor\n",
    "\n",
    "'''Binary trees'''\n",
    "import numpy as np\n",
    "class Tree(object):\n",
    "        def __init__(self, entry, left=None, right=None):\n",
    "            self.entry = entry\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "        def get_entry(self):\n",
    "            return self.entry\n",
    "        def get_left(self):\n",
    "            return self.left\n",
    "        def get_right(self):\n",
    "            return self.right\n",
    "        def __repr__(self):\n",
    "            args = repr(self.entry)\n",
    "            if self.left or self.right:\n",
    "                args += ', {0}, {1}'.format(repr(self.left), repr(self.right))\n",
    "            return 'Tree({0})'.format(args)\n",
    "           \n",
    "        def depth(self):\n",
    "            if self.left == self.right == None:\n",
    "                return 0\n",
    "            elif (self.left !=None)&(self.right !=None):\n",
    "                left_depth = self.left.depth()\n",
    "                right_depth = self.right.depth()\n",
    "                return max(left_depth+1,right_depth+1)\n",
    "        def leaves(self):\n",
    "            if self.left == self.right == None:\n",
    "                return [self.entry]\n",
    "            return self.left.leaves()+self.right.leaves()\n",
    "        def width(self):\n",
    "            return len(self.leaves())\n",
    "        def formula(self):\n",
    "            form = list(str(self.entry))\n",
    "            if self.left or self.right:\n",
    "                form = form + self.right.formula()+[')']\n",
    "                form = ['('] +self.left.formula() + form\n",
    "            return form\n",
    "        def get_nodes(self):  #in the same order as they appear in the formula\n",
    "            form = list(str(self.entry))\n",
    "            if self.left or self.right:\n",
    "                form = form + self.right.get_nodes()\n",
    "                form = self.left.get_nodes() + form\n",
    "            return form\n",
    "        def adj_dim(self):\n",
    "            return len(self.get_nodes())\n",
    "        def absisse_of_left_one(self):\n",
    "            if self.left == None:\n",
    "                return 0\n",
    "            else:\n",
    "                if self.left.left == None:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return self.left.left.adj_dim()\n",
    "        def absisse_of_right_one(self):\n",
    "            if self.right == None:\n",
    "                return 0\n",
    "            else:\n",
    "                if self.right.left == None:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return self.right.left.adj_dim()\n",
    "\n",
    "        def graph(self):\n",
    "            nodes = self.get_nodes()\n",
    "            dim = self.adj_dim()\n",
    "            if dim == 1:\n",
    "                return nodes, np.zeros((1,1))\n",
    "            left_dim = self.left.adj_dim()\n",
    "            right_dim = self.right.adj_dim()\n",
    "            diag_left = self.left.graph()[1]\n",
    "            diag_right = self.right.graph()[1]\n",
    "            haut = np.concatenate((diag_left,np.zeros((left_dim,dim-left_dim))),axis=1)\n",
    "            bas = np.concatenate((np.zeros((right_dim,dim-right_dim)),diag_right),axis=1)\n",
    "            i_left = self.absisse_of_left_one()\n",
    "            i_right = self.absisse_of_right_one()\n",
    "            haut[i_left,left_dim] = 1\n",
    "            bas[i_right, left_dim] = 1\n",
    "            adj = np. concatenate((haut, np.zeros((1,dim)), bas),axis=0)\n",
    "            return nodes, adj\n",
    "\n",
    "        def to_array(self, N, variables, connectors, x=1, a=None, b=None, d=0):  #N = len(array) ; x keeps trace of the horizontal position in the array\n",
    "                                                                            #d of the depth; a is the array for the leaves, b for the rest of the nodes (connectors)\n",
    "            if N<2**(self.depth()+1)-1:\n",
    "                raise Exception(\"Too short array for such tree\")\n",
    "            else:\n",
    "                if a is None:\n",
    "                    a = np.full(N,np.nan)\n",
    "                if b is None:\n",
    "                    b = np.full(N,np.nan)\n",
    "                dic_con = dict(zip(connectors,np.arange(len(connectors))))\n",
    "                idx = 2**(d)+x-2\n",
    "                if (self.right == None)&(self.left==None):   #we suppose the leaves are variables and the other nodes are connectors\n",
    "                    a[idx] = int(self.entry)\n",
    "                    return a,b\n",
    "                else:\n",
    "                    b[idx] = dic_con[self.entry]\n",
    "                    x = 2*(x-1)+1\n",
    "                    r = x+1\n",
    "                    d+=1\n",
    "                    self.left.to_array(N, variables,connectors, x, a, b, d)\n",
    "                    self.right.to_array(N, variables, connectors, r, a, b, d)\n",
    "                    return a,b\n",
    "            \n",
    "        def to_array_parse(self, N, variables, connectors, x=1, a=None, b=None, d=0):  #N = len(array) ; x keeps trace of the horizontal position in the array\n",
    "                                                                            #d of the depth; a is the array for the leaves, b for the rest of the nodes (connectors)\n",
    "            if N<2**(self.depth()+1)-1:\n",
    "                raise Exception(\"Too short array for such tree\")\n",
    "            else:\n",
    "                if a is None:\n",
    "                    a = np.full(N,np.nan)\n",
    "                if b is None:\n",
    "                    b = np.full(N,np.nan)\n",
    "                dic_con = {'l':0,'r':1}\n",
    "                leaves = connectors + variables\n",
    "                dic_leaves = dict(zip(leaves,np.arange(len(leaves))))\n",
    "                idx = 2**(d)+x-2\n",
    "                if (self.right == None)&(self.left==None):   #we suppose the leaves are variables and the other nodes are connectors\n",
    "                    a[idx] = int(dic_leaves[self.entry])\n",
    "                    return a,b\n",
    "                else:\n",
    "                    \n",
    "                    b[idx] = int(dic_con[self.entry])\n",
    "                    x = 2*(x-1)+1\n",
    "                    r = x+1\n",
    "                    d+=1\n",
    "                    self.left.to_array_parse(N, variables,connectors, x, a, b, d)\n",
    "                    self.right.to_array_parse(N, variables, connectors, r, a, b, d)\n",
    "                    return a,b\n",
    "\n",
    "       # def to_list(self,idx=1, a=[], d=0):  #N = len(array) ; idx keeps trace of the position in the array\n",
    "        #    if (self.right == None)&(self.left==None):\n",
    "         #       a.append(int(self.entry))\n",
    "          #      return a\n",
    "           # else:\n",
    "            #   idx = idx+2**d\n",
    "             #   idr = idx+1\n",
    "              #  d+=1\n",
    "               # self.left.to_array(N, idx, a, d)\n",
    "                #self.right.to_array(N, idr, a, d)\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "def tree_generator(variables, connectors, max_depth):\n",
    "    if max_depth == 0:\n",
    "        return Tree(random.choice(variables))\n",
    "    else:\n",
    "        left_depth = random.randint(0,max_depth-1)\n",
    "        right_depth = random.randint(0,max_depth-1)\n",
    "        return Tree(random.choice(connectors),tree_generator(variables, connectors, left_depth),tree_generator(variables, connectors, left_depth))\n",
    "\n",
    "class ArithmeticTree(Tree):\n",
    "    def __init__(self, entry, left=None, right=None):\n",
    "        super().__init__(entry, left, right)\n",
    "    def eval(self):\n",
    "        if self.left == None:\n",
    "            return int(self.entry)\n",
    "        else:\n",
    "            if self.entry == '+':\n",
    "                return self.left.eval() + self.right.eval()\n",
    "            if self.entry == '*':\n",
    "                return self.left.eval() * self.right.eval()\n",
    "            if self.entry == '-':\n",
    "                return self.left.eval() - self.right.eval()\n",
    "\n",
    "def arithmetic_tree_generator(variables, connectors, max_depth):\n",
    "    if max_depth == 0:\n",
    "        return ArithmeticTree(random.choice(variables))\n",
    "    else:\n",
    "        left_depth = random.randint(0,max_depth-1)\n",
    "        right_depth = random.randint(0,max_depth-1)\n",
    "        return ArithmeticTree(random.choice(connectors),arithmetic_tree_generator(variables, connectors, left_depth),arithmetic_tree_generator(variables, connectors, left_depth))\n",
    "\n",
    "'''from arithmetic tree to parse tree'''\n",
    "def translate_Tree(arith_tree):\n",
    "    root = arith_tree.get_entry()\n",
    "    if isinstance(root, str):\n",
    "        root = ArithmeticTree(root)\n",
    "    if arith_tree.get_left()==None:\n",
    "        return arith_tree\n",
    "    else:\n",
    "        left = arith_tree.get_left()\n",
    "        right = arith_tree.get_right()\n",
    "        new_left = translate_Tree(left)\n",
    "        new_right = translate_Tree(right)\n",
    "        return Tree('r',Tree('l',left=new_left,right=root), new_right)\n",
    "        \n",
    "    \n",
    "import networkx as nx\n",
    "def arith_data_gen(variables,connectors, max_depth, n_samples, array=False):\n",
    "    data_set = []\n",
    "    for i in range(n_samples):\n",
    "        tree = arithmetic_tree_generator(variables, connectors, max_depth)\n",
    "        if array == False:\n",
    "            graph = tree.graph()\n",
    "            nodes = graph[0]\n",
    "            adj_mat = graph[1]\n",
    "            y = torch.tensor(tree.eval())\n",
    "            net_graph = nx.from_numpy_array(adj_mat,  create_using=nx.DiGraph)\n",
    "            dic_nodes = dict(zip(list(range(tree.adj_dim())), nodes))\n",
    "            nx.set_node_attributes(net_graph,dic_nodes,'label')\n",
    "            #net_graph.graph['y'] = y\n",
    "            data_set.append((net_graph,y))\n",
    "        else:\n",
    "            N = 2**(max_depth +1)-1\n",
    "            arr = tree.to_array(N, variables, connectors)\n",
    "            y = torch.tensor(tree.eval())\n",
    "            data_set.append((arr,y))\n",
    "    return data_set\n",
    "def arith_data_gen_parse(variables,connectors, max_depth, n_samples):\n",
    "    data_set = []\n",
    "    for i in range(n_samples):\n",
    "        tree = arithmetic_tree_generator(variables, connectors, max_depth)\n",
    "        treet = translate_Tree(tree)\n",
    "        N = 2**(2*max_depth +1)-1\n",
    "        arr = treet.to_array_parse(N, variables, connectors)\n",
    "        y = torch.tensor(tree.eval())\n",
    "        data_set.append((arr,y))\n",
    "    return data_set\n",
    "\n",
    "class One_Hot_nodes(object):\n",
    "    def __init__(self, variables,connectors):\n",
    "        self.l = len(variables+connectors)\n",
    "        self.map = dict(zip(variables+connectors,range(self.l)))\n",
    "\n",
    "    def __call__(self,liste):\n",
    "        self.enc = np.zeros((len(liste),self.l))\n",
    "        for i, node in enumerate(liste):\n",
    "            try :\n",
    "                self.enc[i, self.map[int(node)]] = 1\n",
    "            except :\n",
    "                self.enc[i, self.map[node]] = 1\n",
    "        return torch.from_numpy(self.enc)\n",
    "\n",
    "class One_Hot_Zp(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p               \n",
    "        \n",
    "    def __call__(self,tensor):  #one hot encode the leaves array in Zp\n",
    "        tt=torch.remainder(tensor[0,:],self.p).repeat(self.p,1)\n",
    "        for i in range(tt.shape[1]):\n",
    "            if ~torch.isnan(tt[0,i]):\n",
    "                j = int(tt[0,i])\n",
    "                tt[:,i] = 0\n",
    "                tt[j,i] = 1\n",
    "        return tt,tensor[1,:]\n",
    "\n",
    "class target_Zp(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    def __call__(self,tensor):\n",
    "        return torch.remainder(tensor,self.p)\n",
    "\n",
    "class mod_target_Zp(object):\n",
    "    def __init__(self,p,connectors):\n",
    "        self.p = p\n",
    "        self.move = len(connectors)\n",
    "    def __call__(self,tensor):\n",
    "        return torch.remainder(tensor+self.move,self.p)\n",
    "\n",
    "\n",
    "class target_One_Hot_Zp(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self,tensor):  #one hot encode the leaves array in Zp\n",
    "        tt=torch.remainder(tensor,self.p).repeat(self.p)\n",
    "        j = int(tt[0])\n",
    "        tt[:] = 0\n",
    "        tt[j] = 1\n",
    "        return tt                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset class implementation'''\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
    "class CD_TreeG(Dataset):\n",
    "    def __init__(self, net_data, max_depth, transform=None, target_transform=None):\n",
    "        self.net_data = net_data\n",
    "        self.max_depth = max_depth\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.net_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            if idx < 0 : #Handle negative indices\n",
    "                idx += len( self )\n",
    "            else:\n",
    "                X = torch.tensor(self.net_data[idx][0])\n",
    "                y = self.net_data[idx][1]\n",
    "                if self.transform:\n",
    "                    X = self.transform(X)\n",
    "                if self.target_transform:\n",
    "                    y = self.target_transform(y)\n",
    "\n",
    "            return X, y\n",
    "        elif isinstance(idx, slice ) :\n",
    "            #Get the start, stop, and step from the slice\n",
    "            return [self[ii] for ii in range(*idx.indices(len(self)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45358754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    pos = graphviz_layout(G, prog=\"dot\")\n",
    "    nx.draw_networkx(G, pos=pos, with_labels=True, labels= G.nodes,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*']\n",
    "net_graph = arith_data_gen(variables, connectors, max_depth=4, n_samples =10)\n",
    "visualize_graph(net_graph[3][0],'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604e9c8",
   "metadata": {},
   "source": [
    "# First model - Stochastic Matrices Through Loss Penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f917e",
   "metadata": {},
   "source": [
    "$loss = crossentropy(pred, labels) + \\sum{(\\sum_u{M^u} - 1)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc15ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''M is a stochastic matrix that is learned (only one connector)'''\n",
    "class LearnTree(torch.nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.M = torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p))\n",
    "\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup[0]\n",
    "        arr_conn = tup[1]\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = arr_conn[conn_id]             #will be used when several connectors\n",
    "\n",
    "            prop = torch.einsum('i,uij,j->u',left,self.M,right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(self.M)\n",
    "        return arr_leaves[:,last_leaf_id], self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1000 samples of depth at most 4 and one connector'''\n",
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =1000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "#train_loader = DataLoader(data_train, shuffle=True, batch_size=1)  #as I have no plan to implement batches, this could be a bit of overkill\n",
    "#val_loader = DataLoader(data_val, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b621b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train function'''\n",
    "#the loss needs to add a penalty to (1-\\sum_u {M_u})**2 for each i, j. To keep it stochastic\n",
    "from torch.nn import CrossEntropyLoss\n",
    "criterion = CrossEntropyLoss()\n",
    "lr=10**(-3)                                                    #who knows?\n",
    "#opti = optim.Adam(net1.parameters(), lr=lr, weight_decay=1e-4) #because why not?\n",
    "\n",
    "#device = torch.device(\"cuda:2\")\n",
    "device =torch.device(\"cpu\")\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "\n",
    "            # Computing loss\n",
    "\n",
    "            loss = criterion(logits.squeeze(-1), labels) + torch.sum((torch.sum(M,0) - 1)**2)   #coef? Other expression? Should we fear negative coef?\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                    net_copy = deepcopy(net)\n",
    "                    path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                    torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) + torch.sum((torch.sum(M,0) - 1)**2)\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1].detach()\n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899c3d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net1 = LearnTree(3).to(device)\n",
    "opti = optim.SGD(net1.parameters(), lr=lr)\n",
    "\n",
    "epochs=10\n",
    "train(net1, criterion, opti, lr, train_loader, val_loader, epochs,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b11cc",
   "metadata": {},
   "source": [
    "issue with proba that go out of [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180e686",
   "metadata": {},
   "source": [
    "# URN Model - A Strict Subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd306301",
   "metadata": {},
   "source": [
    "Learn orthogonal matrices instead of stochastic matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb888250",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''p is a natural number and we want to find the $p^3$ matrix that defines a binary fuction/relation'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLearnTreeURN\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,p):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'''p is a natural number and we want to find the $p^3$ matrix that defines a binary fuction/relation'''\n",
    "from torch.nn import Embedding\n",
    "class LearnTreeURN(torch.nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.dim_emb = int(p*(p-1)/2)\n",
    "        self.preM = torch.nn.Parameter(data=torch.full((self.dim_emb, self.p),1/self.p)) # what better initialization?\n",
    "\n",
    "         # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(p,p).long()\n",
    "        for i in range(0,p):\n",
    "            for j in range(i+1,p):\n",
    "                self.ix_mat[i,j] = (i* (2*p - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup[0]\n",
    "        arr_conn = tup[1]\n",
    "        list_ortho = []\n",
    "        for i in range(self.p):\n",
    "            x = self.preM[:,i]\n",
    "            #print(x.shape)\n",
    "            x = torch.cat([torch.zeros(x.shape[:-1]).to(device).unsqueeze(-1), x], dim=-1)\n",
    "            tri = torch.index_select(x, -1, self.ix_mat.flatten()).reshape([self.p,self.p])\n",
    "            tri = tri - tri.transpose(-2, -1)\n",
    "            exp_mat = torch.matrix_exp(tri)\n",
    "            list_ortho.append(exp_mat)\n",
    "        U = torch.stack(list_ortho)  #first we create the ortho mat corresponding to M\n",
    "        U = torch.transpose(U,0,1)   # we need to have distributions along dim0\n",
    "        M = U*U\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = arr_conn[conn_id]             #will be used when several connectors\n",
    "\n",
    "            prop = torch.einsum('i,uij,j->u',left,M,right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(M)\n",
    "        return arr_leaves[:,last_leaf_id], M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b15b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =1000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "#train_loader = DataLoader(data_train, shuffle=True, batch_size=1)  #as I have no plan to implement batches, this could be a bit of overkill\n",
    "#val_loader = DataLoader(data_val, shuffle=True, batch_size=1)\n",
    "\n",
    "#the loss needs to add a penalty to (1-\\sum_u {M_u})**2 for each i, j. To keep it stochastic\n",
    "from torch.nn import CrossEntropyLoss\n",
    "criterion = CrossEntropyLoss()\n",
    "lr=10**(-2)                                                    #who knows?\n",
    "#opti = optim.Adam(net1.parameters(), lr=lr, weight_decay=1e-4) #because why not?\n",
    "\n",
    "#device = torch.device(\"cuda:2\")\n",
    "device =torch.device(\"cpu\")\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "\n",
    "            # Computing loss\n",
    "\n",
    "            loss = criterion(logits.squeeze(-1), labels)    #better!\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                    net_copy = deepcopy(net)\n",
    "                    path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                    torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1].detach()\n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab320eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = LearnTreeURN(3).to(device)\n",
    "opti = optim.SGD(net2.parameters(), lr=lr)\n",
    "\n",
    "epochs=10\n",
    "train(net2, criterion, opti, lr, train_loader, val_loader, epochs,device=device)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f7aef",
   "metadata": {},
   "source": [
    "The 2 last matrices are what we expect, but in the first he does not learn that $0*i=0$. With orth mat, we are imposing that the sum by row should be one. Which is obviously wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871953b2",
   "metadata": {},
   "source": [
    "# Information passing and reversibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dc92b",
   "metadata": {},
   "source": [
    "Multiplication on N+ is reversible in the sense that if you have a and the product of a and b you can find b. But on $Z_p$ it is no more! The 0[p] 'absorbs' information. And the smallest p the more 0s. In some way, this explains why unitary matrices are of no help. They can't learn the line and the row of 1 corresponding to $0*i=0$ and $j*0=0$.\n",
    "How not to make the parallel with microscopic rules being irreversible in quantum meca, while macros are not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = 0\n",
    "for data in (enumerate(data_train)):\n",
    "    if data[1][1].item()!=0:\n",
    "        nonzero+=1\n",
    "nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09aa55a",
   "metadata": {},
   "source": [
    "that is the number of non zero trees in modulo3 among the 800 elements of data_train.\n",
    "The deeper the trees, the smallest will be the ratio of non zeros ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001694a",
   "metadata": {},
   "source": [
    "# Clip Some Weights To 0 And 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad5506",
   "metadata": {},
   "source": [
    "Create 2 class functions that freeze some of the weights to 0 or to 1 when they come close enough. Provided that the lr is small enough, this should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5963c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hypothesis: if some weights come too close to 0... they are 0'''\n",
    "class weightConstraint0(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.parameters():\n",
    "                w = param.data\n",
    "                grad = param.grad.data\n",
    "                mask = w<1/self.p**2\n",
    "                param.data = torch.where(mask,0,w)\n",
    "                param.grad.data = torch.where(mask,0,grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dbd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightConstraint1(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.parameters():\n",
    "                w = param.data\n",
    "                grad = param.grad.data\n",
    "                mask = w>1-1/self.p**2\n",
    "                param.data = torch.where(mask,1,w)\n",
    "                param.grad.data = torch.where(mask,0,grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef01f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip(net, criterion, opti, lr, train_loader, val_loader, epochs,p, device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    constraint0 = weightConstraint0(p)\n",
    "    constraint1 = weightConstraint1(p)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "\n",
    "            # Computing loss\n",
    "\n",
    "            loss = criterion(logits.squeeze(-1), labels) + torch.sum((torch.sum(M,0) - 1)**2)   #coef? Other expression? Should we fear negative coef?\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            net.apply(constraint0)\n",
    "            net.apply(constraint1)\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                #if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                 #   net_copy = deepcopy(net)\n",
    "                  #  path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                   # torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    #print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        print(M)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec30b41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr=10**(-2)\n",
    "net3 = LearnTree(3).to(device)\n",
    "opti = optim.SGD(net3.parameters(), lr=lr)\n",
    "\n",
    "epochs=10\n",
    "train_clip(net3, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a251a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(net3,'parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff05374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in net3.parameters():\n",
    "    param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c05add",
   "metadata": {},
   "source": [
    "# It took 4 epochs to learn mult modulo3 with 800 samples and lr =10**(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99caec",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11a2a7",
   "metadata": {},
   "source": [
    "Bigger lr go faster... but too big will randomly clip some weights to 0 or 1. If gradients are small enough (again...) a magnitude order is $m = d(1/p,1/p^2)=\\frac{p-1}{p^2}$. lr should be less than $m/(k*grad\\_order)$ where k is th minimum number of opti step in the same direction (k=10 looks very safe) and grad_order is the magnitude order of the gradient. As a rule of thumb, we could make a first backward prop and take grad_order as the max absolute value of the first backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54950055",
   "metadata": {},
   "source": [
    "this could be implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382956db",
   "metadata": {},
   "source": [
    "\n",
    "-> it seems that a Bayesian update rule instead of a naive gradient descent would be (in some way to define) optimal. But what do we know about the likelihood? Does it make sense to consider it a Bernoulli (and thus the posterior could be chosen the conjugate: a beta distribution).\n",
    "-> M can be approached by p orthogonal matrices (implementation to do below). Interestingly it would make a proper subspace (all orthognal matrices, when squared element wise are stochastic while not all stochastic matrices can be mapped to orthogonal ones - 2 columns need not be orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c33181",
   "metadata": {},
   "source": [
    "# Many connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnTreeMany(torch.nn.Module):\n",
    "    def __init__(self,p,l):        #l is the number of connectors\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.M = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p)) for i in range(l)])\n",
    "        \n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup[0]\n",
    "        arr_conn = tup[1]\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used now\n",
    "            \n",
    "            prop = torch.einsum('i,uij,j->u',left,self.M[conn],right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(self.M)\n",
    "        return arr_leaves[:,last_leaf_id], self.M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8466527",
   "metadata": {},
   "source": [
    "we add additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =1000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26507c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_graph = arith_data_gen(variables, connectors, max_depth=4, n_samples =10)\n",
    "visualize_graph(net_graph[3][0],'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981827a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net4=LearnTreeMany(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip_many(net, criterion, opti, lr, train_loader, val_loader, epochs,p, device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    constraint0 = weightConstraint0(p)\n",
    "    constraint1 = weightConstraint1(p)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "            l=len(M)\n",
    "            # Computing loss\n",
    "            penalty=0\n",
    "            for i in range(l):\n",
    "                penalty+=torch.sum((torch.sum(M[i],0) - 1)**2)\n",
    "            loss = criterion(logits.squeeze(-1), labels) + penalty   #coef? Other expression? Should we fear negative coef?\n",
    "            #loss = criterion(logits.squeeze(-1), labels)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            net.apply(constraint0)\n",
    "            net.apply(constraint1)\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                #if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                 #   net_copy = deepcopy(net)\n",
    "                  #  path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                   # torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    #print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        print(M[0])\n",
    "        print(M[1])\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            #M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            \n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85debe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hyp: if some weights come too close to 0... they are 0'''\n",
    "class weightConstraint0(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = w<1/self.p**2\n",
    "                    param[1].data = torch.where(mask,0,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) \n",
    "                    \n",
    "\n",
    "class weightConstraint1(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = w>1-1/self.p**2\n",
    "                    param[1].data = torch.where(mask,1,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d30ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr=10**(-2)\n",
    "net4=LearnTreeMany(3,2)\n",
    "opti = optim.SGD(net4.parameters(), lr=lr)\n",
    "\n",
    "epochs=10\n",
    "train_clip_many(net4, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eeedd",
   "metadata": {},
   "source": [
    "It took 7 epochs to learn properly both addition and multiplication randomly mixed in base 3 (with no regulation term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9767637",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1')\n",
    "lr=10**(-2)\n",
    "net6=LearnTreeMany(3,2).to(device)\n",
    "opti = optim.SGD(net6.parameters(), lr=lr)\n",
    "\n",
    "epochs=10\n",
    "train_clip_many(net6, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfbd25",
   "metadata": {},
   "source": [
    "With penalty 5 epochs are enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4495c52",
   "metadata": {},
   "source": [
    "# What if p grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Learning in Zp with p=17,19...'''\n",
    "max_depth = 4\n",
    "p=17\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =1000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1')\n",
    "lr=2*10**(-2)\n",
    "net17=LearnTreeMany(17,2).to(device)\n",
    "opti = optim.SGD(net17.parameters(), lr=lr)\n",
    "\n",
    "epochs=40\n",
    "train_clip_many(net17, criterion, opti, lr, train_loader, val_loader, epochs=80,17,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30307d5",
   "metadata": {},
   "source": [
    "80 epochs and still some mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clip_many(net17, criterion, opti, lr, train_loader, val_loader, epochs=60,17,device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b17a1e",
   "metadata": {},
   "source": [
    "60 epochs and more to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f4a78",
   "metadata": {},
   "source": [
    "Fails. stucks at 68%. Clip doesn't work. The weights fluctuate too much before going to their optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daadc6c",
   "metadata": {},
   "source": [
    "# Softmax + Many connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336d432",
   "metadata": {},
   "source": [
    "Instead of a penalty, we use a softmax to assure stochasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345855b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This time we learn many connectors at the same time. \n",
    "Each corresponds to a stochastic matric in the parameters list'''\n",
    "\n",
    "device = torch.device('cuda:2')\n",
    "class LearnTreeManySoftmax(torch.nn.Module):\n",
    "    def __init__(self,p,l):        #l is the number of connectors\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.M = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p)) for i in range(l)])\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used now\n",
    "            N = self.softmax(self.M[conn])\n",
    "            prop = torch.einsum('i,uij,j->u',left,N,right) #one step of the propagation\n",
    "            #print('prop:',prop)\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(self.M)\n",
    "        return arr_leaves[:,last_leaf_id], self.M\n",
    "\n",
    "\n",
    "\n",
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =1000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "\n",
    "\n",
    "netsoft1=LearnTreeManySoftmax(3,2).to(device)\n",
    "\n",
    "def train_clip_many(net, criterion, opti, lr, train_loader, val_loader, epochs,p, device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    constraint0 = weightConstraint0(p)\n",
    "    constraint1 = weightConstraint1(p)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "            l=len(M)\n",
    "            # Computing loss\n",
    "                        \n",
    "            loss = criterion(logits.squeeze(-1), labels)    #coef? Other expression? Should we fear negative coef?\n",
    "            #loss = criterion(logits.squeeze(-1), labels)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            net.apply(constraint0)\n",
    "            net.apply(constraint1)\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                #if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                 #   net_copy = deepcopy(net)\n",
    "                  #  path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                   # torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    #print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        N0 = nn.Softmax(dim=0)(M[0])\n",
    "        N1 = nn.Softmax(dim=0)(M[1])\n",
    "        #print(N0)\n",
    "        #print(N1)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            \n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count\n",
    "\n",
    "'''hyp: if some weights come too close to 0... they are 0'''\n",
    "class weightConstraint0(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=0)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M<1/self.p**2\n",
    "                    param[1].data = torch.where(mask,-np.inf,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) \n",
    "                    \n",
    "class weightConstraint1(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=0)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M>1-1/self.p  #when p bigger it seems we can release\n",
    "                    param[1].data = torch.where(mask,10,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "netsoft1=LearnTreeManySoftmax(3,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "#opti = optim.Adam(netsoft1.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft1, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf43419",
   "metadata": {},
   "outputs": [],
   "source": [
    "netsoft2=LearnTreeManySoftmax(3,2).to(device)\n",
    "lr=3*10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(netsoft2.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft2, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143c4ac",
   "metadata": {},
   "source": [
    "Adam : 1 epoch learns the rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=17\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =10000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "\n",
    "netsoft3=LearnTreeManySoftmax(17,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(netsoft3.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft3, criterion, opti, lr, train_loader, val_loader, epochs,17,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bc0a7",
   "metadata": {},
   "source": [
    "With 1000 samples, lr 10*-3 semble n'apprendre rien mais 10*-2 semble clipper des valeurs fausses..\n",
    "It seems that the problem is just that we need $n\\_samples>>p^3$ Otherwise the learner does not get enough information to exclude some pathologic possibilities. As we can see here, when we increase n_samples to 10000. 3 epoches are enough to reach 99.65% accuracy in Z17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437a9d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p=19\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen(variables, connectors, max_depth=4, n_samples =10000,array=True)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "\n",
    "netsoft19=LearnTreeManySoftmax(19,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(netsoft19.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft19, criterion, opti, lr, train_loader, val_loader, epochs,19,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cc4e0",
   "metadata": {},
   "source": [
    "Z19 in the pocket!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f66b3",
   "metadata": {},
   "source": [
    "# The NL Tree Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6c9b3",
   "metadata": {},
   "source": [
    "The idea now is to set the connectors and variables on the same level (as leaves int the tree), while the nodes will be assigned some values corresponding to pregroup types reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b04ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from arithmetic tree to parse tree'''\n",
    "def translate_Tree(arith_tree):\n",
    "    root = arith_tree.get_entry()\n",
    "    if isinstance(root, str):\n",
    "        root = ArithmeticTree(root)\n",
    "    if arith_tree.get_left()==None:\n",
    "        return arith_tree\n",
    "    else:\n",
    "        left = arith_tree.get_left()\n",
    "        right = arith_tree.get_right()\n",
    "        new_left = translate_Tree(left)\n",
    "        new_right = translate_Tree(right)\n",
    "        return Tree('r',Tree('l',left=new_left,right=root), new_right)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2730a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2', '*', '3', '+', '5'],\n",
       " array([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree= ArithmeticTree('+',ArithmeticTree('*',ArithmeticTree(2),ArithmeticTree(3)),ArithmeticTree(5))\n",
    "tree.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f787b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tree.graph()\n",
    "nodes = graph[0]\n",
    "adj_mat = graph[1]\n",
    "y = torch.tensor(tree.eval())\n",
    "net_graph = nx.from_numpy_array(adj_mat,  create_using=nx.DiGraph)\n",
    "dic_nodes = dict(zip(list(range(tree.adj_dim())), nodes))\n",
    "nx.set_node_attributes(net_graph,dic_nodes,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e960986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "visualize_graph(net_graph,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea434c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree('r', Tree('l', Tree('r', Tree('l', Tree(2), Tree('*')), Tree(3)), Tree('+')), Tree(5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treet = translate_Tree(tree)\n",
    "treet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a7fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = treet.graph()\n",
    "nodes = graph[0]\n",
    "adj_mat = graph[1]\n",
    "\n",
    "net_graph = nx.from_numpy_array(adj_mat,  create_using=nx.DiGraph)\n",
    "dic_nodes = dict(zip(list(range(treet.adj_dim())), nodes))\n",
    "nx.set_node_attributes(net_graph,dic_nodes,'label')\n",
    "\n",
    "\n",
    "visualize_graph(net_graph,'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94375d1a",
   "metadata": {},
   "source": [
    "This time we need to encode in N both the leaves values that are numbers, but also those that are connectors. We do it by mapping the connectors to {0,1} and the numbers to n+2 (as 0 and 1 are already taken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34719c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectors=['*','+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b64007",
   "metadata": {},
   "outputs": [],
   "source": [
    "treet.to_array_parse(32,variables, connectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bc796",
   "metadata": {},
   "source": [
    "left and right correspond to left and right reductions, the only 2 kinds that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02284e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen_parse(variables, connectors, max_depth=4, n_samples =1000)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca37872",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "class LearnTreeManySoftmax(torch.nn.Module):\n",
    "    def __init__(self,p,l):        #l is the number of connectors\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.M = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p)) for i in range(l)])\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used now\n",
    "            N = self.softmax(self.M[conn])\n",
    "            prop = torch.einsum('i,uij,j->u',left,N,right) #one step of the propagation\n",
    "            #print('prop:',prop)\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(self.M)\n",
    "        return arr_leaves[:,last_leaf_id], self.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "netsoft1=LearnTreeManySoftmax(3,2).to(device)\n",
    "\n",
    "def train_clip_many(net, criterion, opti, lr, train_loader, val_loader, epochs,p, device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    constraint0 = weightConstraint0(p)\n",
    "    constraint1 = weightConstraint1(p)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "            l=len(M)\n",
    "            # Computing loss\n",
    "                        \n",
    "            loss = criterion(logits.squeeze(-1), labels)    #coef? Other expression? Should we fear negative coef?\n",
    "            #loss = criterion(logits.squeeze(-1), labels)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            net.apply(constraint0)\n",
    "            net.apply(constraint1)\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                #if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                 #   net_copy = deepcopy(net)\n",
    "                  #  path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                   # torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    #print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        N0 = nn.Softmax(dim=0)(M[0])\n",
    "        N1 = nn.Softmax(dim=0)(M[1])\n",
    "        #print(N0)\n",
    "        #print(N1)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            \n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count\n",
    "\n",
    "'''hyp: if some weights come too close to 0... they are 0'''\n",
    "class weightConstraint0(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=0)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M<1/self.p**2\n",
    "                    param[1].data = torch.where(mask,-np.inf,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) \n",
    "                    \n",
    "class weightConstraint1(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=0)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M>1-1/self.p  #when p bigger it seems we can release\n",
    "                    param[1].data = torch.where(mask,10,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "netsoft2=LearnTreeManySoftmax(3,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(netsoft2.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft2, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e344662",
   "metadata": {},
   "source": [
    "It gets stuck.. And my intuition is that $M^u$ is not stochastic anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437c8ac",
   "metadata": {},
   "source": [
    "#  Make It URN Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41433423",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "\n",
    "'''p is a natural number and we want to find the $p^3$ matrix that defines a binary fuction/relation'''\n",
    "from torch.nn import Embedding\n",
    "class LearnTreeURN_many(torch.nn.Module):\n",
    "    def __init__(self,p,l):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.l = l\n",
    "        self.dim_emb = int(p*(p-1)/2)\n",
    "        self.preM = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.dim_emb,self.p),1/self.p)) for i in range(l)])\n",
    "         # what better initialization?\n",
    "\n",
    "         # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(p,p).long()\n",
    "        for i in range(0,p):\n",
    "            for j in range(i+1,p):\n",
    "                self.ix_mat[i,j] = (i* (2*p - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        \n",
    "        U=[]\n",
    "        for j in range(self.l):\n",
    "            list_ortho = []\n",
    "            mat = self.preM[j]\n",
    "            for i in range(self.p):\n",
    "                x = mat[:,i]\n",
    "                #print(x.shape)\n",
    "                x = torch.cat([torch.zeros(x.shape[:-1]).to(device).unsqueeze(-1), x], dim=-1)\n",
    "                tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape([self.p,self.p])\n",
    "                tri = tri - tri.transpose(-2, -1)\n",
    "                exp_mat = torch.matrix_exp(tri)\n",
    "                list_ortho.append(exp_mat)\n",
    "            O = torch.stack(list_ortho)  #first we create the ortho mat corresponding to M\n",
    "            #if j == 0:\n",
    "             #   O = torch.transpose(O,0,1)   # we need to have distributions along dim0\n",
    "            if j == 1:\n",
    "                O = torch.transpose(O,0,2)\n",
    "            U.append(O)\n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used when several connectors\n",
    "            M = U[conn]\n",
    "            \n",
    "            prop = torch.einsum('i,uij,j->u',left,M,right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(M)\n",
    "        return arr_leaves[:,last_leaf_id], M\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            #M = out[1]\n",
    "\n",
    "            # Computing loss\n",
    "\n",
    "            loss = criterion(logits.squeeze(-1), labels)    #better!\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                \n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            #M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            #M = out[1].detach()\n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0015ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neturn2=LearnTreeURN_many(3,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(neturn2.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train(neturn2, criterion, opti, lr, train_loader, val_loader, epochs,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732de629",
   "metadata": {},
   "source": [
    "The rules are not learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b935e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one tries to learn an ortho mat for + and a stochastic for *\n",
    "\n",
    "from torch.nn import Embedding\n",
    "class LearnTreehalfURN_many(torch.nn.Module):\n",
    "    def __init__(self,p,l):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.l = l\n",
    "        self.dim_emb = int(p*(p-1)/2)\n",
    "        self.preO = torch.nn.Parameter(data=torch.full((self.dim_emb,self.p),1/self.p)) \n",
    "        self.M = torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p)) \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "         # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(p,p).long()\n",
    "        for i in range(0,p):\n",
    "            for j in range(i+1,p):\n",
    "                self.ix_mat[i,j] = (i* (2*p - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        list_ortho =[]\n",
    "       \n",
    "        for i in range(self.p):\n",
    "            x = self.preO[:,i]\n",
    "            #print(x.shape)\n",
    "            x = torch.cat([torch.zeros(x.shape[:-1]).to(device).unsqueeze(-1), x], dim=-1)\n",
    "            tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape([self.p,self.p])\n",
    "            tri = tri - tri.transpose(-2, -1)\n",
    "            exp_mat = torch.matrix_exp(tri)\n",
    "            list_ortho.append(exp_mat)\n",
    "        O = torch.stack(list_ortho)  #first we create the ortho mat corresponding to M\n",
    "        O = torch.transpose(O,0,2)\n",
    "        \n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used when several connectors\n",
    "            if conn == 0 :\n",
    "                N = O\n",
    "            if conn == 1 :\n",
    "                N = self.softmax(self.M)\n",
    "            prop = torch.einsum('i,uij,j->u',left,N,right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(M)\n",
    "        return arr_leaves[:,last_leaf_id], N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9687c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "neturn3=LearnTreehalfURN_many(3,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(neturn3.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train(neturn3, criterion, opti, lr, train_loader, val_loader, epochs,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e56585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "class LearnTreehalfURN_many(torch.nn.Module):\n",
    "    def __init__(self,p,l):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.l = l\n",
    "        self.dim_emb = int(p*(p-1)/2)\n",
    "        self.preO = torch.nn.Parameter(data=torch.full((self.dim_emb,self.p),1/self.p)) \n",
    "        self.M = torch.nn.Parameter(data=torch.full((self.p,self.p,self.p),1/self.p)) \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "         # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(p,p).long()\n",
    "        for i in range(0,p):\n",
    "            for j in range(i+1,p):\n",
    "                self.ix_mat[i,j] = (i* (2*p - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        list_ortho =[]\n",
    "       \n",
    "        for i in range(self.p):\n",
    "            x = self.preO[:,i]\n",
    "            #print(x.shape)\n",
    "            x = torch.cat([torch.zeros(x.shape[:-1]).to(device).unsqueeze(-1), x], dim=-1)\n",
    "            tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape([self.p,self.p])\n",
    "            tri = tri - tri.transpose(-2, -1)\n",
    "            exp_mat = torch.matrix_exp(tri)\n",
    "            list_ortho.append(exp_mat)\n",
    "        O = torch.stack(list_ortho)  #first we create the ortho mat corresponding to M\n",
    "        O = torch.transpose(O,0,2)\n",
    "        \n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used when several connectors\n",
    "            if conn == 0 :\n",
    "                N = O\n",
    "            if conn == 1 :\n",
    "                N = self.softmax(self.M)\n",
    "            prop = torch.einsum('i,uij,j->u',left,N,right) #one step of the propagation\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(M)\n",
    "        return arr_leaves[:,last_leaf_id], N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0453355",
   "metadata": {},
   "source": [
    "# A mixture of ortho and degenerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2acbad",
   "metadata": {},
   "source": [
    "Learn matrices of rank 1 (generated by 1 vector $\\ket{v}\\bra{v}$) and orthogonal matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "\n",
    "'''p is a natural number and we want to find the $p^3$ matrix that defines a binary fuction/relation'''\n",
    "from torch.nn import Embedding\n",
    "class LearnParse(torch.nn.Module):\n",
    "    def __init__(self,p,l):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.l = l\n",
    "        self.dim_emb = int(p*(p-1)/2)\n",
    "        self.preM = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.dim_emb,self.p),1/self.p)) for i in range(l)])\n",
    "        self.Degen = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.p,self.p),1/self.p)) for i in range(l)]) #to learn the pos of the column vector\n",
    "        self.u = nn.ParameterList([torch.nn.Parameter(data=torch.full((self.p,self.p),1/self.p)) for i in range(l)]) #to learn the unitary vectors\n",
    "         \n",
    "         # for creating the upper tringulars\n",
    "        self.ix_mat = torch.zeros(p,p).long()\n",
    "        for i in range(0,p):\n",
    "            for j in range(i+1,p):\n",
    "                self.ix_mat[i,j] = (i* (2*p - i - 3))//2 + j - 1 + 1\n",
    "\n",
    "        list_lin = [nn.Linear(p,1) for i in range(l)]    #One for each connector\n",
    "        self.list_linear = nn.ModuleList(list_lin)       #the attention layer\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_conn = tup_tens[1]\n",
    "        \n",
    "        U=[]\n",
    "        D=[]\n",
    "        for j in range(self.l):\n",
    "            list_ortho = []\n",
    "            mat = self.preM[j]\n",
    "            list_degen =[]\n",
    "            vec = self.Degen[j]\n",
    "            uni = self.u[j]\n",
    "            \n",
    "            \n",
    "            for i in range(self.p):\n",
    "                x = mat[:,i]\n",
    "                #print(x.shape)\n",
    "                x = torch.cat([torch.zeros(x.shape[:-1]).to(device).unsqueeze(-1), x], dim=-1)\n",
    "                tri = torch.index_select(x, -1, self.ix_mat.flatten().to(device)).reshape([self.p,self.p])\n",
    "                tri = tri - tri.transpose(-2, -1)\n",
    "                exp_mat = torch.matrix_exp(tri)\n",
    "                list_ortho.append(exp_mat)\n",
    "                \n",
    "                v = vec[i,:].unsqueeze(0)\n",
    "                u = self.softmax(uni[i,:]).unsqueeze(1)\n",
    "                deg_mat = u@v   #a column of norm one\n",
    "                list_degen.append(deg_mat)\n",
    "                #print(\"list_degen:\",list_degen)\n",
    "            O = torch.stack(list_ortho)  #first we create the ortho mat corresponding to M\n",
    "            O = torch.transpose(O,0,2)\n",
    "            U.append(O)\n",
    "            #print(\"U:\",U)\n",
    "            G = torch.stack(list_degen)\n",
    "            G = torch.transpose(G,0,2)\n",
    "            D.append(G) \n",
    "            \n",
    "            \n",
    "        while torch.sum(1-(arr_leaves[0,:].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1].squeeze().float()\n",
    "            right = arr_leaves[:,last_leaf_id].squeeze().float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used when several connectors\n",
    "            M = U[conn]\n",
    "            N = D[conn]\n",
    "            linear = self.list_linear[conn]\n",
    "            t = linear(left)\n",
    "            t = torch.sigmoid(t)\n",
    "            #print('t',t)\n",
    "            prop1 = torch.einsum('i,uij,j->u',left,M,right) #one step of the propagation\n",
    "            prop2 = torch.einsum('i,uij,j->u',left,N,right)\n",
    "            prop = t*prop1 + (1-t)*prop2  #t is a function of conn and of left in [0,1]. Hopefully values of 0 and 1 will quickly be reached\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:].isnan().long()+2).nonzero())\n",
    "        #print(M)\n",
    "        return arr_leaves[:,last_leaf_id], M\n",
    "\n",
    "#Les problemes possibles:\n",
    "#1 confusion dans les dimensions transposees\n",
    "#2 la couche d'attention: est-ce assez d'avoir une fonction lineaire en x (on veut apprendre une fonction qui retourne (1,0) pour certaines valeurs de x (0,1) pour d'autres\n",
    "#3 Les clips functions devraient etre modifiees pour clipper t!\n",
    "#4 Est ce que nos matrices ortho definies comme $exp(T-T^t)$ where T are reel triangular sup can reach all permutations ? (JP \n",
    "#disait, je crois, que non... Pourtant...)\n",
    "\n",
    "\n",
    "def train(net, criterion, opti, lr, train_loader, val_loader, epochs,device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            #M = out[1]\n",
    "\n",
    "            # Computing loss\n",
    "\n",
    "            loss = criterion(logits.squeeze(-1), labels)    #better!\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                \n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            #M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            #M = out[1].detach()\n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen_parse(variables, connectors, max_depth=4, n_samples =1000)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = target_Zp(p))\n",
    "train_loader = data_train\n",
    "val_loader =data_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4db1c6",
   "metadata": {},
   "source": [
    "Still not here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96456707",
   "metadata": {},
   "source": [
    "# 4 dims for certains connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "p=3\n",
    "variables = list(np.arange(10))\n",
    "connectors = ['*','+']\n",
    "net_data = arith_data_gen_parse(variables, connectors, max_depth=4, n_samples =1000)\n",
    "l = len(net_data)\n",
    "l_train =round(.8*l)\n",
    "net_train = net_data[:l_train]\n",
    "net_val = net_data[l_train:]\n",
    "data_train = CD_TreeG(net_train, max_depth, transform = One_Hot_Zp(p), target_transform = mod_target_Zp(p,connectors))\n",
    "data_val = CD_TreeG(net_val, max_depth, transform = One_Hot_Zp(p), target_transform = mod_target_Zp(p,connectors))\n",
    "train_loader = data_train\n",
    "val_loader =data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[11][0][0].unsqueeze(-1).repeat(1,1,3).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')\n",
    "class LearnTreeManySoftmax(torch.nn.Module):\n",
    "    def __init__(self,p,l):        #l is the number of connectors\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.Ml =torch.nn.Parameter(data=torch.full((self.p,self.p,self.p,self.p),1/self.p))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, tup_tens):\n",
    "        #tup = deepcopy(tup_tens)\n",
    "        arr_leaves = tup_tens[0]\n",
    "        arr_leaves = arr_leaves.unsqueeze(-1).repeat(1,1,self.p)\n",
    "        #print(arr_leaves.size())\n",
    "        arr_conn = tup_tens[1]\n",
    "        while torch.sum(1-(arr_leaves[0,:,0].isnan()*1))!= 1: #loop as long as we have more than one leaf\n",
    "\n",
    "            #print((1-(arr_leaves[0,:].isnan()*1)))\n",
    "            last_leaf_id = max((1-(arr_leaves[0,:,0].isnan()*1)).nonzero())  #just an argmax that returns the max argument...\n",
    "            #print('lastleafid:',last_leaf_id)\n",
    "            d = floor(math.log2(last_leaf_id+1))   # get the depth of the leaf\n",
    "            #print('d',d)\n",
    "            x = last_leaf_id+2-2**d                   # get the horizontal pos\n",
    "            #print('x:',x)\n",
    "            conn_id = int(2**(d-1) + (x/2)-2)           # get the index of the corresponding connector\n",
    "            #print('conn_id',conn_id)\n",
    "            left = arr_leaves[:,last_leaf_id-1,:].float()\n",
    "            #print('left before comp',left.size())\n",
    "            right = arr_leaves[:,last_leaf_id,:].float()\n",
    "            conn = int(arr_conn[conn_id].item())             #used now\n",
    "            if conn==0:\n",
    "                \n",
    "                N = self.softmax(self.Ml)\n",
    "                left = left[:,:,0].squeeze()\n",
    "                right = right[:,:,0].squeeze()\n",
    "                #print(left.size())\n",
    "                prop = torch.einsum('i,vuij,j->vu',left,N,right) #one step of the propagation\n",
    "            else :\n",
    "                right = right[:,:,0].squeeze()\n",
    "                prop = left@right\n",
    "                prop = prop.repeat(1,1,self.p)\n",
    "            #print('prop:',prop)\n",
    "            #print('arr_leaves before update', arr_leaves)\n",
    "            #print('arr_conn before update', arr_conn)\n",
    "            arr_leaves[:,last_leaf_id-1,:] = float('nan')\n",
    "            arr_leaves[:,last_leaf_id,:] = float('nan')\n",
    "            arr_conn[conn_id] = float('nan')\n",
    "            arr_leaves[:,conn_id,:] = prop\n",
    "            #print('arr_leaves after update', arr_leaves)\n",
    "            #print('arr_conn after update', arr_conn)\n",
    "        last_leaf_id = max((~arr_leaves[0,:,0].isnan().long()+2).nonzero())\n",
    "        #print(self.M)\n",
    "        return arr_leaves[:,last_leaf_id,0],self.Ml\n",
    "\n",
    "netsoft1=LearnTreeManySoftmax(3,2).to(device)\n",
    "\n",
    "def train_clip_many(net, criterion, opti, lr, train_loader, val_loader, epochs,p, device=device):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_train_loss = np.Inf\n",
    "    rl_prec = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    constraint0 = weightConstraint0(p)\n",
    "    constraint1 = weightConstraint1(p)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (tup_tens, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            # Converting to cuda tensors\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "\n",
    "            # logits from the model\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "\n",
    "            #param from the model\n",
    "            M = out[1]\n",
    "            l=len(M)\n",
    "            # Computing loss\n",
    "                        \n",
    "            loss = criterion(logits.squeeze(-1), labels)    #coef? Other expression? Should we fear negative coef?\n",
    "            #loss = criterion(logits.squeeze(-1), labels)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            net.apply(constraint0)\n",
    "            net.apply(constraint1)\n",
    "            opti.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                #if (val_losses==[]) & (rl_prec>running_loss):\n",
    "                 #   net_copy = deepcopy(net)\n",
    "                  #  path_to_model='LearnTree.pt'.format(net.__class__.__name__) #save more often for the server disconnections\n",
    "                   # torch.save(net_copy.state_dict(), path_to_model)\n",
    "                    #print(\"The model has been saved in {}\".format(path_to_model))\n",
    "                rl_prec = running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        val_loss = eval_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        acc = accuracy(net, device, val_loader)\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "        print(\"Accuracy : {}\".format(acc))\n",
    "        N0 = nn.Softmax(dim=0)(M[0])\n",
    "        N1 = nn.Softmax(dim=0)(M[1])\n",
    "        #print(N0)\n",
    "        #print(N1)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "\n",
    "\n",
    "def eval_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            M = out[1]\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels) \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def accuracy(net, device, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    right = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (tup_tens, labels) in enumerate(dataloader):\n",
    "            tup_tens, labels = (tup_tens[0].to(device), tup_tens[1].to(device)), labels.to(device)\n",
    "            out = net(tup_tens)\n",
    "            logits = out[0]\n",
    "            \n",
    "            pred = torch.argmax(logits)\n",
    "\n",
    "            right += int(pred==labels)               #number of correct predictions\n",
    "            count += 1\n",
    "\n",
    "    return right / count\n",
    "\n",
    "'''hyp: if some weights come too close to 0... they are 0'''\n",
    "class weightConstraint0(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=1)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M<1/self.p**2\n",
    "                    param[1].data = torch.where(mask,-np.inf,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) \n",
    "                    \n",
    "class weightConstraint1(object):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'parameters'):\n",
    "            for param in module.named_parameters():\n",
    "                w = param[1].data\n",
    "                if param[1].grad is not None:\n",
    "                    M = nn.Softmax(dim=1)(w)\n",
    "                    grad = param[1].grad.data\n",
    "                    mask = M>1-1/self.p  #when p bigger it seems we can release\n",
    "                    param[1].data = torch.where(mask,10,w)\n",
    "                    param[1].grad.data = torch.where(mask,0,grad) \n",
    "\n",
    "netsoft2=LearnTreeManySoftmax(3,2).to(device)\n",
    "lr=10**(-2)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "#opti = optim.SGD(netsoft1.parameters(), lr=lr)\n",
    "\n",
    "opti = optim.Adam(netsoft2.parameters(),lr =lr)\n",
    "epochs=10\n",
    "train_clip_many(netsoft2, criterion, opti, lr, train_loader, val_loader, epochs,3,device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
